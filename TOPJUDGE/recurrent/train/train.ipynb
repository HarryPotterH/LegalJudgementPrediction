{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pickle as pk\n",
    "from Topjudge_model import Topjudge\n",
    "from parser_ import ConfigParser\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from law_processed import get_law_graph\n",
    "import keras.layers\n",
    "#from tensorflow.contrib import layers  版本问题，2.x版本tensorflow没有对应的包， 使用下面那个替代\n",
    "from tf_slim import layers\n",
    "from LSTM_cell import LSTMDecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=1e-9\n",
    "\n",
    "\n",
    "def gen_dict(inputs_, law_labels_input_, accu_labels_input_, time_labels_input_):\n",
    "\n",
    "    feed_dict_ = {fact_input: inputs_, law_labels: law_labels_input_,\n",
    "                  accu_labels: accu_labels_input_, time_labels: time_labels_input_}\n",
    "\n",
    "    return feed_dict_\n",
    "\n",
    "\n",
    "def evaluation_multitask(y, prediction, task_num, correct_tags, total_tags):\n",
    "    accuracy_ = []\n",
    "    metrics_acc = []\n",
    "    for x in range(task_num):\n",
    "        accuracy_1 = correct_tags[x] / total_tags * 100\n",
    "        accuracy_metric = metrics.accuracy_score(y[x], prediction[x])\n",
    "        macro_recall = metrics.recall_score(y[x], prediction[x], average='macro')\n",
    "        micro_recall = metrics.recall_score(y[x], prediction[x], average='micro')\n",
    "        macro_precision = metrics.precision_score(y[x], prediction[x], average='macro')\n",
    "        micro_precision = metrics.precision_score(y[x], prediction[x], average='micro')\n",
    "        macro_f1 = metrics.f1_score(y[x], prediction[x], average='macro')\n",
    "        micro_f1 = metrics.f1_score(y[x], prediction[x], average='micro')\n",
    "        accuracy_.append(accuracy_1)\n",
    "        metrics_acc.append(\n",
    "            (accuracy_metric, macro_recall, micro_recall, macro_precision, micro_precision, macro_f1, micro_f1))\n",
    "    return accuracy_, metrics_acc\n",
    "\n",
    "\n",
    "def get_safe_shift(logits, mask):\n",
    "    \"\"\"\n",
    "    :param logits: A tf.Tensor of shape [B, TQ, TK] of dtype tf.float32\n",
    "    :param mask: A tf.Tensor of shape [B, TQ, TK] of dtype tf.float32\n",
    "    where TQ, TK are the maximum lengths of the queries resp. the keys in the batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine minimum\n",
    "    K_shape=logits.get_shape().as_list()\n",
    "    mask_shape=mask.get_shape().as_list()\n",
    "    if mask_shape!=K_shape:\n",
    "        mask=tf.tile(mask,[1]+[K_shape[1]//mask_shape[1]]+[1]*(len(K_shape)-2))\n",
    "\n",
    "    logits_min = tf.reduce_min(logits, axis=-1, keepdims=True)      # [B, TQ, 1]\n",
    "    logits_min = tf.tile(logits_min, multiples=[1]*(len(K_shape)-1)+[K_shape[-1]])  # [B, TQ, TK]\n",
    "\n",
    "    logits = tf.where(condition=mask > .5, x=logits, y=logits_min)\n",
    "\n",
    "    # Determine maximum\n",
    "    logits_max = tf.reduce_max(logits, axis=-1, keepdims=True, name=\"logits_max\")      # [B, TQ, 1]\n",
    "    logits_shifted = tf.subtract(logits, logits_max, name=\"logits_shifted\")    # [B, TQ, TK]\n",
    "\n",
    "    return logits_shifted\n",
    "\n",
    "\n",
    "def padding_aware_softmax(logits, key_mask, query_mask=None):\n",
    "\n",
    "    logits_shifted=get_safe_shift(logits, key_mask)\n",
    "\n",
    "    # Apply exponential\n",
    "    weights_unscaled = tf.exp(logits_shifted)\n",
    "\n",
    "    # Apply mask\n",
    "    weights_unscaled = tf.multiply(key_mask, weights_unscaled)     # [B, TQ, TK]\n",
    "\n",
    "    # Derive total mass\n",
    "    weights_total_mass = tf.reduce_sum(weights_unscaled, axis=-1, keepdims=True)     # [B, TQ, 1]\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if query_mask:\n",
    "        weights_total_mass = tf.where(condition=tf.equal(query_mask, 1),\n",
    "                                    x=weights_total_mass,\n",
    "                                    y=tf.ones_like(weights_total_mass))\n",
    "\n",
    "    # Normalize weights\n",
    "    weights = tf.divide(weights_unscaled, weights_total_mass + epsilon)   # [B, TQ, TK]\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def atten_encoder_mask(Q, K, fc_layer=None, mask=None, weights_regularizer=None, K_ori=False, div_norm=True):\n",
    "    '''\n",
    "    :param Q: [..., seq_len_q, F] : the attention vector u\n",
    "    :param K: [..., seq_len_k, F]\n",
    "    :param mask:\n",
    "    :return: a tensor whose size is [..., F]\n",
    "    '''\n",
    "    V = K\n",
    "    K_shape = K.get_shape().as_list() # size[x, y, z]\n",
    "    if fc_layer is not None:\n",
    "        K = fc_layer(K)\n",
    "    else:\n",
    "        K = layers.fully_connected(K, K_shape[-1], activation_fn=tf.nn.tanh, weights_regularizer=weights_regularizer)  # size: [x, y, z]\n",
    "\n",
    "    if not K_ori:\n",
    "        V = K\n",
    "    # ======================================================\n",
    "    # Q=tf.transpose(Q,[-1,-2])\n",
    "    # scores=tf.map_fn(lambda x:x@Q,K,dtype=tf.float32)\n",
    "    # -------------another implementation-------------------\n",
    "    scores = tf.reduce_sum(K * Q, -1)  # size: [x, y]\n",
    "    if div_norm:\n",
    "        scores = scores/tf.sqrt(tf.cast(K_shape[-1],tf.float32))  # size: [x, y]\n",
    "    # =======================================================\n",
    "    # scores=tf.nn.softmax(scores,-2)\n",
    "    if mask is not None:\n",
    "        # scores = scores - tf.reduce_max(scores, -1, keepdims=True)  # e^(a-b)=e^a * e^-b\n",
    "        # exp = tf.exp(scores) * mask  # big scores cause inf in exp\n",
    "        # scores = exp / (tf.reduce_sum(exp, -1, keepdims=True) + epsilon)  # the score is attention weight a , size: [x, y]\n",
    "        scores = padding_aware_softmax(scores, mask)\n",
    "    else:\n",
    "        scores = tf.nn.softmax(scores, -1)\n",
    "    return tf.reduce_sum(tf.expand_dims(scores, -1) * V, -2), scores  # size: [x, z]\n",
    "\n",
    "\n",
    "def run_model(input, mask, model):\n",
    "    input_shape = input.get_shape().as_list()\n",
    "    mask = tf.expand_dims(mask, -1)\n",
    "    mask_shape = mask.get_shape().as_list()\n",
    "    input = tf.reshape(input, [int(np.prod(input_shape[:-2]))] + input_shape[-2:])\n",
    "    mask = tf.reshape(mask, [int(np.prod(mask_shape[:-2]))] + mask_shape[-2:])\n",
    "    out = model(input, mask=mask)\n",
    "    rep = tf.reshape(out, input_shape[:-1] + [lstm_size * 2])\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configFilePath = 'default.config'\n",
    "config = ConfigParser(configFilePath)\n",
    "'''\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', '-c')\n",
    "parser.add_argument('--gpu', '-g')\n",
    "args = parser.parse_args()\n",
    "\n",
    "'''\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "max_epoch = 16\n",
    "sent_len_fact = 100\n",
    "doc_len_fact = 15\n",
    "doc_len_law = 10\n",
    "sent_len_law = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "lstm_size = 128\n",
    "clr_fc1_size = 512\n",
    "clr_fc2_size = 256\n",
    "law_relation_threshold = 0.3\n",
    "\n",
    "\n",
    "task = ['law', 'accu', 'time']\n",
    "with open('../data/w2id_thulac.pkl', 'rb') as f:\n",
    "    word2id_dict = pk.load(f)\n",
    "    f.close()\n",
    "\n",
    "emb_path = '../data/cail_thulac.npy'\n",
    "word_embedding = np.cast[np.float32](np.load(emb_path))\n",
    "\n",
    "word_dict_len = len(word2id_dict)\n",
    "vec_size = 200\n",
    "shuffle = True\n",
    "\n",
    "n_law = 103\n",
    "n_accu = 119\n",
    "n_term = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 5263.45 (49.474 sec)\n",
      "Step 1: loss = 5258.93 (100.283 sec)\n",
      "Step 2: loss = 5255.86 (158.137 sec)\n",
      "Step 3: loss = 5250.53 (213.814 sec)\n",
      "Step 4: loss = 5244.84 (265.820 sec)\n",
      "Step 5: loss = 5239.75 (323.952 sec)\n",
      "Step 6: loss = 5235.12 (378.639 sec)\n",
      "Step 7: loss = 5230.22 (433.264 sec)\n",
      "Step 8: loss = 5224.32 (486.220 sec)\n",
      "Step 9: loss = 5220.50 (540.123 sec)\n",
      "Step 10: loss = 5217.30 (598.594 sec)\n",
      "Step 11: loss = 5214.04 (653.549 sec)\n",
      "Step 12: loss = 5209.28 (711.128 sec)\n",
      "Step 13: loss = 5205.00 (762.110 sec)\n",
      "Step 14: loss = 5200.48 (820.272 sec)\n",
      "Step 15: loss = 5195.18 (868.704 sec)\n",
      "Step 16: loss = 5189.99 (920.846 sec)\n",
      "Step 17: loss = 5185.28 (976.701 sec)\n",
      "Step 18: loss = 5179.38 (1026.935 sec)\n",
      "Step 19: loss = 5176.33 (1075.127 sec)\n",
      "Step 20: loss = 5171.51 (1125.453 sec)\n",
      "Step 21: loss = 5165.75 (1181.075 sec)\n",
      "Step 22: loss = 5160.89 (1234.202 sec)\n",
      "Step 23: loss = 5155.99 (1289.605 sec)\n",
      "Step 24: loss = 5152.65 (1348.041 sec)\n",
      "Step 25: loss = 5148.31 (1399.522 sec)\n",
      "Step 26: loss = 5143.47 (1452.061 sec)\n",
      "Step 27: loss = 5138.97 (1502.756 sec)\n",
      "Step 28: loss = 5133.92 (1553.852 sec)\n",
      "Step 29: loss = 5130.15 (1607.235 sec)\n",
      "Step 30: loss = 5124.99 (1656.951 sec)\n",
      "Step 31: loss = 5119.85 (1714.787 sec)\n",
      "Step 32: loss = 5115.48 (1771.091 sec)\n",
      "Step 33: loss = 5112.41 (1829.853 sec)\n",
      "Step 34: loss = 5108.05 (1878.466 sec)\n",
      "Step 35: loss = 5104.41 (1938.151 sec)\n",
      "Step 36: loss = 5099.87 (1995.313 sec)\n",
      "Step 37: loss = 5095.47 (2052.091 sec)\n",
      "Step 38: loss = 5092.19 (2103.437 sec)\n",
      "Step 39: loss = 5088.47 (2152.718 sec)\n",
      "Step 40: loss = 5083.27 (2206.417 sec)\n",
      "Step 41: loss = 5077.43 (2259.842 sec)\n",
      "Step 42: loss = 5071.67 (2314.583 sec)\n",
      "Step 43: loss = 5066.77 (2366.713 sec)\n",
      "Step 44: loss = 5062.13 (2426.486 sec)\n",
      "Step 45: loss = 5057.06 (2476.631 sec)\n",
      "Step 46: loss = 5053.65 (2528.426 sec)\n",
      "Step 47: loss = 5048.47 (2576.625 sec)\n",
      "Step 48: loss = 5045.46 (2626.675 sec)\n",
      "Step 49: loss = 5041.22 (2682.293 sec)\n",
      "Step 50: loss = 5037.64 (2735.093 sec)\n",
      "Step 51: loss = 5033.83 (2788.031 sec)\n",
      "Step 52: loss = 5028.68 (2841.323 sec)\n",
      "Step 53: loss = 5025.53 (2900.462 sec)\n",
      "Step 54: loss = 5019.63 (2957.115 sec)\n",
      "Step 55: loss = 5014.20 (3007.074 sec)\n",
      "Step 56: loss = 5010.83 (3056.050 sec)\n",
      "Step 57: loss = 5006.45 (3111.385 sec)\n",
      "Step 58: loss = 5002.75 (3165.559 sec)\n",
      "Step 59: loss = 4998.14 (3217.724 sec)\n",
      "Step 60: loss = 4992.95 (3274.190 sec)\n",
      "Step 61: loss = 4989.41 (3326.126 sec)\n",
      "Step 62: loss = 4985.85 (3385.498 sec)\n",
      "Step 63: loss = 4981.83 (3440.625 sec)\n",
      "Step 64: loss = 4977.28 (3489.432 sec)\n",
      "Step 65: loss = 4973.50 (3538.899 sec)\n",
      "Step 66: loss = 4967.86 (3589.017 sec)\n",
      "Step 67: loss = 4963.43 (3647.966 sec)\n",
      "Step 68: loss = 4958.90 (3703.219 sec)\n",
      "Step 69: loss = 4955.61 (3760.314 sec)\n",
      "Step 70: loss = 4951.35 (3816.309 sec)\n",
      "Step 71: loss = 4948.10 (3872.775 sec)\n",
      "Step 72: loss = 4944.04 (3931.709 sec)\n",
      "Step 73: loss = 4938.61 (3983.791 sec)\n",
      "Step 74: loss = 4935.22 (4040.991 sec)\n",
      "Step 75: loss = 4930.51 (4097.356 sec)\n",
      "Step 76: loss = 4926.55 (4145.878 sec)\n",
      "Step 77: loss = 4922.72 (4198.689 sec)\n",
      "Step 78: loss = 4916.82 (4258.604 sec)\n",
      "Step 79: loss = 4911.65 (4316.264 sec)\n",
      "Step 80: loss = 4906.03 (4373.579 sec)\n",
      "Step 81: loss = 4901.13 (4425.802 sec)\n",
      "Step 82: loss = 4895.59 (4480.617 sec)\n",
      "Step 83: loss = 4891.87 (4529.215 sec)\n",
      "Step 84: loss = 4887.95 (4579.704 sec)\n",
      "Step 85: loss = 4884.78 (4630.857 sec)\n",
      "Step 86: loss = 4880.56 (4689.468 sec)\n",
      "Step 87: loss = 4875.28 (4746.404 sec)\n",
      "Step 88: loss = 4869.60 (4797.184 sec)\n",
      "Step 89: loss = 4864.54 (4846.273 sec)\n",
      "Step 90: loss = 4861.07 (4898.739 sec)\n",
      "Step 91: loss = 4858.01 (4956.346 sec)\n",
      "Step 92: loss = 4853.72 (5010.067 sec)\n",
      "Step 93: loss = 4848.12 (5059.441 sec)\n",
      "Step 94: loss = 4843.72 (5107.899 sec)\n",
      "Step 95: loss = 4840.50 (5158.874 sec)\n",
      "Step 96: loss = 4835.28 (5211.069 sec)\n",
      "Step 97: loss = 4830.75 (5259.925 sec)\n",
      "Step 98: loss = 4827.42 (5318.121 sec)\n",
      "Step 99: loss = 4823.15 (5368.213 sec)\n",
      "Step 100: loss = 4819.76 (5418.627 sec)\n",
      "Step 101: loss = 4814.11 (5467.896 sec)\n",
      "Step 102: loss = 4808.59 (5520.321 sec)\n",
      "Step 103: loss = 4804.36 (5568.917 sec)\n",
      "Step 104: loss = 4800.88 (5617.200 sec)\n",
      "Step 105: loss = 4795.01 (5667.113 sec)\n",
      "Step 106: loss = 4789.19 (5715.515 sec)\n",
      "Step 107: loss = 4785.69 (5763.526 sec)\n",
      "Step 108: loss = 4781.90 (5812.829 sec)\n",
      "Step 109: loss = 4777.12 (5869.965 sec)\n",
      "Step 110: loss = 4771.34 (5920.381 sec)\n",
      "Step 111: loss = 4766.30 (5969.955 sec)\n",
      "Step 112: loss = 4760.98 (6029.557 sec)\n",
      "Step 113: loss = 4756.45 (6088.773 sec)\n",
      "Step 114: loss = 4750.64 (6143.853 sec)\n",
      "Step 115: loss = 4746.91 (6192.020 sec)\n",
      "Step 116: loss = 4742.15 (6251.116 sec)\n",
      "Step 117: loss = 4736.80 (6303.065 sec)\n",
      "Step 118: loss = 4733.30 (6351.777 sec)\n",
      "Step 119: loss = 4727.53 (6407.641 sec)\n",
      "Step 120: loss = 4723.54 (6461.598 sec)\n",
      "Step 121: loss = 4719.07 (6516.458 sec)\n",
      "Step 122: loss = 4714.52 (6570.356 sec)\n",
      "Step 123: loss = 4708.95 (6625.690 sec)\n",
      "Step 124: loss = 4703.40 (6679.514 sec)\n",
      "Step 125: loss = 4698.50 (6733.399 sec)\n",
      "Step 126: loss = 4692.91 (6793.386 sec)\n",
      "Step 127: loss = 4687.83 (6842.177 sec)\n",
      "Step 128: loss = 4683.12 (6890.262 sec)\n",
      "Step 129: loss = 4677.97 (6941.110 sec)\n",
      "Step 130: loss = 4673.95 (6994.044 sec)\n",
      "Step 131: loss = 4670.27 (7049.994 sec)\n",
      "Step 132: loss = 4664.35 (7105.514 sec)\n",
      "Step 133: loss = 4661.01 (7157.793 sec)\n",
      "Step 134: loss = 4655.16 (7214.906 sec)\n",
      "Step 135: loss = 4650.43 (7265.338 sec)\n",
      "Step 136: loss = 4645.97 (7319.563 sec)\n",
      "Step 137: loss = 4640.89 (7376.109 sec)\n",
      "Step 138: loss = 4636.60 (7426.518 sec)\n",
      "Step 139: loss = 4631.55 (7478.193 sec)\n",
      "Step 140: loss = 4626.38 (7533.914 sec)\n",
      "Step 141: loss = 4621.33 (7590.641 sec)\n",
      "Step 142: loss = 4616.45 (7640.282 sec)\n",
      "Step 143: loss = 4612.30 (7688.307 sec)\n",
      "Step 144: loss = 4609.30 (7741.448 sec)\n",
      "Step 145: loss = 4606.25 (7789.584 sec)\n",
      "Step 146: loss = 4602.24 (7845.771 sec)\n",
      "Step 147: loss = 4596.69 (7899.186 sec)\n",
      "Step 148: loss = 4592.42 (7953.985 sec)\n",
      "Step 149: loss = 4588.82 (8008.690 sec)\n",
      "Step 150: loss = 4584.83 (8064.693 sec)\n",
      "Step 151: loss = 4578.99 (8122.663 sec)\n",
      "Step 152: loss = 4573.55 (8178.815 sec)\n",
      "Step 153: loss = 4569.13 (8237.679 sec)\n",
      "Step 154: loss = 4565.29 (8295.073 sec)\n",
      "Step 155: loss = 4560.06 (8345.188 sec)\n",
      "Step 156: loss = 4555.98 (8397.886 sec)\n",
      "Step 157: loss = 4551.41 (8446.667 sec)\n",
      "Step 158: loss = 4546.12 (8499.374 sec)\n",
      "Step 159: loss = 4541.68 (8548.298 sec)\n",
      "Step 160: loss = 4538.42 (8605.080 sec)\n",
      "Step 161: loss = 4532.81 (8663.759 sec)\n",
      "Step 162: loss = 4526.89 (8720.493 sec)\n",
      "Step 163: loss = 4521.41 (8774.630 sec)\n",
      "Step 164: loss = 4516.23 (8833.371 sec)\n",
      "Step 165: loss = 4510.34 (8887.133 sec)\n",
      "Step 166: loss = 4505.04 (8938.723 sec)\n",
      "Step 167: loss = 4501.58 (8996.499 sec)\n",
      "Step 168: loss = 4496.08 (9047.027 sec)\n",
      "Step 169: loss = 4492.08 (9098.088 sec)\n",
      "Step 170: loss = 4486.17 (9147.276 sec)\n",
      "Step 171: loss = 4481.24 (9196.058 sec)\n",
      "Step 172: loss = 4476.23 (9251.237 sec)\n",
      "Step 173: loss = 4472.80 (9307.956 sec)\n",
      "Step 174: loss = 4468.99 (9358.186 sec)\n",
      "Step 175: loss = 4463.46 (9406.523 sec)\n",
      "Step 176: loss = 4458.64 (9462.522 sec)\n",
      "Step 177: loss = 4452.96 (9512.719 sec)\n",
      "Step 178: loss = 4447.55 (9572.548 sec)\n",
      "Step 179: loss = 4441.62 (9630.586 sec)\n",
      "Step 180: loss = 4436.20 (9681.024 sec)\n",
      "Step 181: loss = 4431.08 (9738.084 sec)\n",
      "Step 182: loss = 4427.27 (9787.759 sec)\n",
      "Step 183: loss = 4423.18 (9842.437 sec)\n",
      "Step 184: loss = 4417.88 (9890.735 sec)\n",
      "Step 185: loss = 4413.82 (9939.613 sec)\n",
      "Step 186: loss = 4409.97 (9989.457 sec)\n",
      "Step 187: loss = 4406.11 (10044.625 sec)\n",
      "Step 188: loss = 4402.43 (10096.410 sec)\n",
      "Step 189: loss = 4398.43 (10146.232 sec)\n",
      "Step 190: loss = 4393.35 (10205.410 sec)\n",
      "Step 191: loss = 4388.39 (10261.171 sec)\n",
      "Step 192: loss = 4385.31 (10318.956 sec)\n",
      "Step 193: loss = 4381.95 (10374.883 sec)\n",
      "Step 194: loss = 4376.22 (10424.582 sec)\n",
      "Step 195: loss = 4370.66 (10477.308 sec)\n",
      "Step 196: loss = 4366.53 (10533.763 sec)\n",
      "Step 197: loss = 4363.32 (10589.485 sec)\n",
      "Step 198: loss = 4358.75 (10645.384 sec)\n",
      "Step 199: loss = 4355.63 (10694.620 sec)\n",
      "Step 200: loss = 4350.95 (10753.518 sec)\n",
      "Step 201: loss = 4347.06 (10807.901 sec)\n",
      "Step 202: loss = 4343.50 (10867.164 sec)\n",
      "Step 203: loss = 4338.14 (10922.546 sec)\n",
      "Step 204: loss = 4334.52 (10981.489 sec)\n",
      "Step 205: loss = 4330.54 (11033.051 sec)\n",
      "Step 206: loss = 4324.78 (11091.299 sec)\n",
      "Step 207: loss = 4321.78 (11150.714 sec)\n",
      "Step 208: loss = 4315.82 (11208.693 sec)\n",
      "Step 209: loss = 4312.45 (11268.129 sec)\n",
      "Step 210: loss = 4307.54 (11317.139 sec)\n",
      "Step 211: loss = 4303.39 (11370.073 sec)\n",
      "Step 212: loss = 4300.05 (11423.554 sec)\n",
      "Step 213: loss = 4295.23 (11478.914 sec)\n",
      "Step 214: loss = 4290.84 (11531.757 sec)\n",
      "Step 215: loss = 4286.64 (11587.027 sec)\n",
      "Step 216: loss = 4281.07 (11644.481 sec)\n",
      "Step 217: loss = 4278.05 (11694.010 sec)\n",
      "Step 218: loss = 4272.22 (11750.828 sec)\n",
      "Step 219: loss = 4268.38 (11805.790 sec)\n",
      "Step 220: loss = 4263.90 (11861.782 sec)\n",
      "Step 221: loss = 4259.56 (11915.194 sec)\n",
      "Step 222: loss = 4254.71 (11966.872 sec)\n",
      "Step 223: loss = 4248.93 (12021.117 sec)\n",
      "Step 224: loss = 4244.37 (12074.877 sec)\n",
      "Step 225: loss = 4238.75 (12130.669 sec)\n",
      "Step 226: loss = 4234.53 (12180.870 sec)\n",
      "Step 227: loss = 4229.06 (12231.694 sec)\n",
      "Step 228: loss = 4224.63 (12282.260 sec)\n",
      "Step 229: loss = 4219.91 (12332.105 sec)\n",
      "Step 230: loss = 4214.15 (12382.862 sec)\n",
      "Step 231: loss = 4209.20 (12432.546 sec)\n",
      "Step 232: loss = 4204.67 (12484.424 sec)\n",
      "Step 233: loss = 4201.09 (12534.644 sec)\n",
      "Step 234: loss = 4196.67 (12592.585 sec)\n",
      "Step 235: loss = 4193.06 (12642.242 sec)\n",
      "Step 236: loss = 4188.59 (12696.658 sec)\n",
      "Step 237: loss = 4185.13 (12747.220 sec)\n",
      "Step 238: loss = 4180.83 (12800.507 sec)\n",
      "Step 239: loss = 4175.75 (12848.621 sec)\n",
      "Step 240: loss = 4170.33 (12905.867 sec)\n",
      "Step 241: loss = 4164.78 (12965.681 sec)\n",
      "Step 242: loss = 4161.38 (13016.409 sec)\n",
      "Step 243: loss = 4158.15 (13073.402 sec)\n",
      "Step 244: loss = 4154.75 (13132.223 sec)\n",
      "Step 245: loss = 4149.79 (13183.654 sec)\n",
      "Step 246: loss = 4145.60 (13238.317 sec)\n",
      "Step 247: loss = 4141.57 (13288.880 sec)\n",
      "Step 248: loss = 4137.54 (13343.298 sec)\n",
      "Step 249: loss = 4131.62 (13397.234 sec)\n",
      "Step 250: loss = 4127.63 (13452.372 sec)\n",
      "Step 251: loss = 4122.92 (13503.506 sec)\n",
      "Step 252: loss = 4117.29 (13559.723 sec)\n",
      "Step 253: loss = 4113.39 (13618.682 sec)\n",
      "Step 254: loss = 4108.41 (13668.144 sec)\n",
      "Step 255: loss = 4104.00 (13725.059 sec)\n",
      "Step 256: loss = 4099.04 (13779.265 sec)\n",
      "Step 257: loss = 4094.89 (13829.433 sec)\n",
      "Step 258: loss = 4090.05 (13886.878 sec)\n",
      "Step 259: loss = 4085.55 (13940.346 sec)\n",
      "Step 260: loss = 4081.74 (13992.617 sec)\n",
      "Step 261: loss = 4078.08 (14047.512 sec)\n",
      "Step 262: loss = 4074.75 (14099.236 sec)\n",
      "Step 263: loss = 4071.46 (14147.717 sec)\n",
      "Step 264: loss = 4066.37 (14203.534 sec)\n",
      "Step 265: loss = 4060.80 (14257.414 sec)\n",
      "Step 266: loss = 4054.89 (14308.007 sec)\n",
      "Step 267: loss = 4048.99 (14362.469 sec)\n",
      "Step 268: loss = 4044.55 (14422.227 sec)\n",
      "Step 269: loss = 4041.38 (14470.959 sec)\n",
      "Step 270: loss = 4037.58 (14530.380 sec)\n",
      "Step 271: loss = 4033.81 (14584.625 sec)\n",
      "Step 272: loss = 4028.91 (14635.216 sec)\n",
      "Step 273: loss = 4023.93 (14688.595 sec)\n",
      "Step 274: loss = 4018.13 (14741.451 sec)\n",
      "Step 275: loss = 4013.48 (14798.147 sec)\n",
      "Step 276: loss = 4010.30 (14854.896 sec)\n",
      "Step 277: loss = 4006.03 (14911.397 sec)\n",
      "Step 278: loss = 4000.93 (14962.446 sec)\n",
      "Step 279: loss = 3997.59 (15014.364 sec)\n",
      "Step 280: loss = 3991.85 (15070.917 sec)\n",
      "Step 281: loss = 3986.71 (15130.113 sec)\n",
      "Step 282: loss = 3982.36 (15179.967 sec)\n",
      "Step 283: loss = 3977.96 (15234.963 sec)\n",
      "Step 284: loss = 3972.18 (15293.729 sec)\n",
      "Step 285: loss = 3967.73 (15353.229 sec)\n",
      "Step 286: loss = 3962.45 (15405.464 sec)\n",
      "Step 287: loss = 3956.83 (15463.854 sec)\n",
      "Step 288: loss = 3951.60 (15518.284 sec)\n",
      "Step 289: loss = 3947.89 (15570.025 sec)\n",
      "Step 290: loss = 3942.54 (15622.286 sec)\n",
      "Step 291: loss = 3936.92 (15678.127 sec)\n",
      "Step 292: loss = 3933.67 (15733.679 sec)\n",
      "Step 293: loss = 3929.36 (15783.628 sec)\n",
      "Step 294: loss = 3925.59 (15836.319 sec)\n",
      "Step 295: loss = 3922.58 (15890.260 sec)\n",
      "Step 296: loss = 3917.44 (15949.266 sec)\n",
      "Step 297: loss = 3911.86 (15997.520 sec)\n",
      "Step 298: loss = 3906.44 (16053.082 sec)\n",
      "Step 299: loss = 3903.36 (16102.733 sec)\n",
      "Step 300: loss = 3899.24 (16161.798 sec)\n",
      "Step 301: loss = 3894.59 (16220.942 sec)\n",
      "Step 302: loss = 3891.21 (16279.362 sec)\n",
      "Step 303: loss = 3887.08 (16329.491 sec)\n",
      "Step 304: loss = 3881.50 (16385.950 sec)\n",
      "Step 305: loss = 3877.11 (16445.442 sec)\n",
      "Step 306: loss = 3871.52 (16503.515 sec)\n",
      "Step 307: loss = 3865.73 (16555.422 sec)\n",
      "Step 308: loss = 3861.90 (16615.282 sec)\n",
      "Step 309: loss = 3857.29 (16668.842 sec)\n",
      "Step 310: loss = 3853.97 (16718.459 sec)\n",
      "Step 311: loss = 3848.80 (16772.598 sec)\n",
      "Step 312: loss = 3845.23 (16827.514 sec)\n",
      "Step 313: loss = 3840.32 (16881.341 sec)\n",
      "Step 314: loss = 3835.48 (16940.409 sec)\n",
      "Step 315: loss = 3830.65 (16993.093 sec)\n",
      "Step 316: loss = 3827.09 (17042.941 sec)\n",
      "Step 317: loss = 3822.01 (17095.106 sec)\n",
      "Step 318: loss = 3817.22 (17143.275 sec)\n",
      "Step 319: loss = 3813.92 (17198.373 sec)\n",
      "Step 320: loss = 3810.34 (17252.001 sec)\n",
      "Step 321: loss = 3804.49 (17301.092 sec)\n",
      "Step 322: loss = 3798.89 (17358.673 sec)\n",
      "Step 323: loss = 3793.19 (17413.766 sec)\n",
      "Step 324: loss = 3789.56 (17466.744 sec)\n",
      "Step 325: loss = 3785.17 (17523.595 sec)\n",
      "Step 326: loss = 3779.80 (17573.025 sec)\n",
      "Step 327: loss = 3774.94 (17624.888 sec)\n",
      "Step 328: loss = 3771.00 (17673.940 sec)\n",
      "Step 329: loss = 3766.11 (17728.621 sec)\n",
      "Step 330: loss = 3761.42 (17781.092 sec)\n",
      "Step 331: loss = 3758.30 (17830.531 sec)\n",
      "Step 332: loss = 3754.15 (17884.173 sec)\n",
      "Step 333: loss = 3750.10 (17943.593 sec)\n",
      "Step 334: loss = 3745.12 (17992.995 sec)\n",
      "Step 335: loss = 3740.37 (18047.387 sec)\n",
      "Step 336: loss = 3736.92 (18106.117 sec)\n",
      "Step 337: loss = 3732.09 (18160.150 sec)\n",
      "Step 338: loss = 3728.20 (18211.402 sec)\n",
      "Step 339: loss = 3722.49 (18267.711 sec)\n",
      "Step 340: loss = 3717.75 (18318.928 sec)\n",
      "Step 341: loss = 3712.43 (18369.867 sec)\n",
      "Step 342: loss = 3707.50 (18419.714 sec)\n",
      "Step 343: loss = 3701.91 (18477.123 sec)\n",
      "Step 344: loss = 3697.97 (18532.428 sec)\n",
      "Step 345: loss = 3692.42 (18589.068 sec)\n",
      "Step 346: loss = 3689.02 (18642.766 sec)\n",
      "Step 347: loss = 3683.84 (18692.775 sec)\n",
      "Step 348: loss = 3679.97 (18743.709 sec)\n",
      "Step 349: loss = 3674.18 (18799.046 sec)\n",
      "Step 350: loss = 3669.74 (18847.926 sec)\n",
      "Step 351: loss = 3666.06 (18897.142 sec)\n",
      "Step 352: loss = 3660.22 (18955.728 sec)\n",
      "Step 353: loss = 3655.26 (19008.305 sec)\n",
      "Step 354: loss = 3651.19 (19062.331 sec)\n",
      "Step 355: loss = 3645.29 (19118.897 sec)\n",
      "Step 356: loss = 3641.09 (19168.737 sec)\n",
      "Step 357: loss = 3635.38 (19218.236 sec)\n",
      "Step 358: loss = 3632.30 (19277.880 sec)\n",
      "Step 359: loss = 3627.04 (19326.207 sec)\n",
      "Step 360: loss = 3623.93 (19374.430 sec)\n",
      "Step 361: loss = 3620.32 (19430.265 sec)\n",
      "Step 362: loss = 3615.16 (19490.134 sec)\n",
      "Step 363: loss = 3611.28 (19542.188 sec)\n",
      "Step 364: loss = 3605.95 (19591.481 sec)\n",
      "Step 365: loss = 3602.68 (19642.566 sec)\n",
      "Step 366: loss = 3596.83 (19695.978 sec)\n",
      "Step 367: loss = 3593.29 (19748.252 sec)\n",
      "Step 368: loss = 3588.70 (19801.984 sec)\n",
      "Step 369: loss = 3583.18 (19861.209 sec)\n",
      "Step 370: loss = 3578.44 (19918.287 sec)\n",
      "Step 371: loss = 3574.91 (19966.614 sec)\n",
      "Step 372: loss = 3569.64 (20018.938 sec)\n",
      "Step 373: loss = 3565.33 (20076.004 sec)\n",
      "Step 374: loss = 3560.50 (20127.535 sec)\n",
      "Step 375: loss = 3555.74 (20178.948 sec)\n",
      "Step 376: loss = 3550.21 (20233.418 sec)\n",
      "Step 377: loss = 3546.49 (20287.383 sec)\n",
      "Step 378: loss = 3541.56 (20344.229 sec)\n",
      "Step 379: loss = 3537.36 (20400.409 sec)\n",
      "Step 380: loss = 3531.44 (20454.742 sec)\n",
      "Step 381: loss = 3526.07 (20510.370 sec)\n",
      "Step 382: loss = 3522.30 (20558.374 sec)\n",
      "Step 383: loss = 3516.88 (20609.120 sec)\n",
      "Step 384: loss = 3512.23 (20665.931 sec)\n",
      "Step 385: loss = 3507.93 (20720.308 sec)\n",
      "Step 386: loss = 3504.61 (20779.216 sec)\n",
      "Step 387: loss = 3500.52 (20828.117 sec)\n",
      "Step 388: loss = 3496.08 (20881.163 sec)\n",
      "Step 389: loss = 3492.43 (20933.380 sec)\n",
      "Step 390: loss = 3488.96 (20990.605 sec)\n",
      "Step 391: loss = 3485.32 (21039.538 sec)\n",
      "Step 392: loss = 3482.07 (21090.097 sec)\n",
      "Step 393: loss = 3476.60 (21143.536 sec)\n",
      "Step 394: loss = 3470.93 (21197.453 sec)\n",
      "Step 395: loss = 3466.93 (21253.698 sec)\n",
      "Step 396: loss = 3461.97 (21307.151 sec)\n",
      "Step 397: loss = 3456.13 (21360.336 sec)\n",
      "Step 398: loss = 3452.71 (21409.933 sec)\n",
      "Step 399: loss = 3449.36 (21458.988 sec)\n",
      "Step 400: loss = 3443.77 (21512.970 sec)\n",
      "Step 401: loss = 3438.82 (21572.190 sec)\n",
      "Step 402: loss = 3435.24 (21631.800 sec)\n",
      "Step 403: loss = 3431.05 (21689.468 sec)\n",
      "Step 404: loss = 3427.50 (21748.127 sec)\n",
      "Step 405: loss = 3422.06 (21801.082 sec)\n",
      "Step 406: loss = 3416.08 (21859.645 sec)\n",
      "Step 407: loss = 3412.91 (21916.937 sec)\n",
      "Step 408: loss = 3408.05 (21976.055 sec)\n",
      "Step 409: loss = 3404.84 (22035.893 sec)\n",
      "Step 410: loss = 3401.64 (22085.594 sec)\n",
      "Step 411: loss = 3396.18 (22139.295 sec)\n",
      "Step 412: loss = 3390.30 (22197.305 sec)\n",
      "Step 413: loss = 3385.70 (22254.927 sec)\n",
      "Step 414: loss = 3382.29 (22311.592 sec)\n",
      "Step 415: loss = 3376.58 (22366.019 sec)\n",
      "Step 416: loss = 3370.59 (22417.072 sec)\n",
      "Step 417: loss = 3366.66 (22465.592 sec)\n",
      "Step 418: loss = 3362.49 (22516.786 sec)\n",
      "Step 419: loss = 3356.56 (22573.424 sec)\n",
      "Step 420: loss = 3350.76 (22627.593 sec)\n",
      "Step 421: loss = 3347.69 (22683.258 sec)\n",
      "Step 422: loss = 3343.48 (22741.124 sec)\n",
      "Step 423: loss = 3338.45 (22798.979 sec)\n",
      "Step 424: loss = 3334.53 (22856.022 sec)\n",
      "Step 425: loss = 3330.16 (22914.753 sec)\n",
      "Step 426: loss = 3324.71 (22966.059 sec)\n",
      "Step 427: loss = 3320.84 (23018.255 sec)\n",
      "Step 428: loss = 3315.44 (23068.404 sec)\n",
      "Step 429: loss = 3311.47 (23119.102 sec)\n",
      "Step 430: loss = 3307.21 (23169.050 sec)\n",
      "Step 431: loss = 3302.97 (23220.487 sec)\n",
      "Step 432: loss = 3297.53 (23273.781 sec)\n",
      "Step 433: loss = 3291.68 (23332.057 sec)\n",
      "Step 434: loss = 3288.38 (23383.445 sec)\n",
      "Step 435: loss = 3283.56 (23437.269 sec)\n",
      "Step 436: loss = 3278.67 (23488.406 sec)\n",
      "Step 437: loss = 3274.15 (23548.070 sec)\n",
      "Step 438: loss = 3268.38 (23596.361 sec)\n",
      "Step 439: loss = 3262.73 (23652.805 sec)\n",
      "Step 440: loss = 3258.81 (23710.761 sec)\n",
      "Step 441: loss = 3253.64 (23768.774 sec)\n",
      "Step 442: loss = 3248.86 (23819.557 sec)\n",
      "Step 443: loss = 3245.23 (23873.366 sec)\n",
      "Step 444: loss = 3241.48 (23929.134 sec)\n",
      "Step 445: loss = 3236.34 (23977.852 sec)\n",
      "Step 446: loss = 3230.63 (24034.520 sec)\n",
      "Step 447: loss = 3225.05 (24087.434 sec)\n",
      "Step 448: loss = 3220.07 (24145.284 sec)\n",
      "Step 449: loss = 3214.82 (24199.937 sec)\n",
      "Step 450: loss = 3210.50 (24251.072 sec)\n",
      "Step 451: loss = 3205.77 (24310.724 sec)\n",
      "Step 452: loss = 3199.96 (24361.109 sec)\n",
      "Step 453: loss = 3196.94 (24418.481 sec)\n",
      "Step 454: loss = 3192.92 (24476.944 sec)\n",
      "Step 455: loss = 3189.26 (24532.291 sec)\n",
      "Step 456: loss = 3183.82 (24592.186 sec)\n",
      "Step 457: loss = 3178.26 (24651.376 sec)\n",
      "Step 458: loss = 3174.99 (24708.311 sec)\n",
      "Step 459: loss = 3169.79 (24762.280 sec)\n",
      "Step 460: loss = 3164.59 (24818.416 sec)\n",
      "Step 461: loss = 3160.14 (24870.632 sec)\n",
      "Step 462: loss = 3154.78 (24926.891 sec)\n",
      "Step 463: loss = 3150.47 (24978.008 sec)\n",
      "Step 464: loss = 3146.74 (25026.188 sec)\n",
      "Step 465: loss = 3142.12 (25075.242 sec)\n",
      "Step 466: loss = 3136.50 (25132.496 sec)\n",
      "Step 467: loss = 3131.13 (25189.690 sec)\n",
      "Step 468: loss = 3125.33 (25246.061 sec)\n",
      "Step 469: loss = 3120.19 (25296.009 sec)\n",
      "Step 470: loss = 3115.00 (25355.703 sec)\n",
      "Step 471: loss = 3110.99 (25405.128 sec)\n",
      "Step 472: loss = 3106.33 (25454.268 sec)\n",
      "Step 473: loss = 3102.34 (25508.674 sec)\n",
      "Step 474: loss = 3099.01 (25557.476 sec)\n",
      "Step 475: loss = 3095.50 (25612.906 sec)\n",
      "Step 476: loss = 3091.49 (25660.922 sec)\n",
      "Step 477: loss = 3085.72 (25717.744 sec)\n",
      "Step 478: loss = 3081.36 (25769.324 sec)\n",
      "Step 479: loss = 3075.36 (25817.336 sec)\n",
      "Step 480: loss = 3071.34 (25868.071 sec)\n",
      "Step 481: loss = 3067.51 (25921.011 sec)\n",
      "Step 482: loss = 3061.71 (25980.172 sec)\n",
      "Step 483: loss = 3058.31 (26036.648 sec)\n",
      "Step 484: loss = 3053.03 (26090.764 sec)\n",
      "Step 485: loss = 3049.22 (26138.889 sec)\n",
      "Step 486: loss = 3046.01 (26190.886 sec)\n",
      "Step 487: loss = 3042.39 (26249.879 sec)\n",
      "Step 488: loss = 3037.53 (26308.467 sec)\n",
      "Step 489: loss = 3032.80 (26356.919 sec)\n",
      "Step 490: loss = 3028.02 (26415.945 sec)\n",
      "Step 491: loss = 3024.78 (26474.049 sec)\n",
      "Step 492: loss = 3019.95 (26532.770 sec)\n",
      "Step 493: loss = 3014.29 (26584.360 sec)\n",
      "Step 494: loss = 3010.74 (26635.037 sec)\n",
      "Step 495: loss = 3006.43 (26689.583 sec)\n",
      "Step 496: loss = 3002.69 (26746.490 sec)\n",
      "Step 497: loss = 2998.44 (26799.973 sec)\n",
      "Step 498: loss = 2992.99 (26854.456 sec)\n",
      "Step 499: loss = 2989.64 (26910.183 sec)\n",
      "Step 500: loss = 2984.41 (26960.509 sec)\n",
      "Step 501: loss = 2978.68 (27014.774 sec)\n",
      "Step 502: loss = 2973.35 (27071.827 sec)\n",
      "Step 503: loss = 2967.64 (27121.113 sec)\n",
      "Step 504: loss = 2962.33 (27169.620 sec)\n",
      "Step 505: loss = 2958.29 (27229.438 sec)\n",
      "Step 506: loss = 2953.21 (27286.485 sec)\n",
      "Step 507: loss = 2949.96 (27337.856 sec)\n",
      "Step 508: loss = 2945.72 (27389.882 sec)\n",
      "Step 509: loss = 2942.69 (27447.074 sec)\n",
      "Step 510: loss = 2938.82 (27506.230 sec)\n",
      "Step 511: loss = 2933.09 (27555.685 sec)\n",
      "Step 512: loss = 2928.64 (27607.719 sec)\n",
      "Step 513: loss = 2925.34 (27657.604 sec)\n",
      "Step 514: loss = 2919.65 (27709.086 sec)\n",
      "Step 515: loss = 2915.60 (27765.338 sec)\n",
      "Step 516: loss = 2912.44 (27821.926 sec)\n",
      "Step 517: loss = 2908.13 (27871.165 sec)\n",
      "Step 518: loss = 2902.24 (27926.267 sec)\n",
      "Step 519: loss = 2898.43 (27975.677 sec)\n",
      "Step 520: loss = 2894.65 (28027.996 sec)\n",
      "Step 521: loss = 2890.71 (28079.014 sec)\n",
      "Step 522: loss = 2885.01 (28134.782 sec)\n",
      "Step 523: loss = 2880.26 (28185.543 sec)\n",
      "Step 524: loss = 2874.62 (28237.224 sec)\n",
      "Step 525: loss = 2870.44 (28288.896 sec)\n",
      "Step 526: loss = 2867.08 (28346.863 sec)\n",
      "Step 527: loss = 2862.70 (28395.258 sec)\n",
      "Step 528: loss = 2858.05 (28445.799 sec)\n",
      "Step 529: loss = 2854.54 (28493.889 sec)\n",
      "Step 530: loss = 2848.93 (28545.059 sec)\n",
      "Step 531: loss = 2845.78 (28598.742 sec)\n",
      "Step 532: loss = 2840.23 (28649.696 sec)\n",
      "Step 533: loss = 2837.05 (28703.310 sec)\n",
      "Step 534: loss = 2831.65 (28763.040 sec)\n",
      "Step 535: loss = 2826.12 (28819.869 sec)\n",
      "Step 536: loss = 2822.67 (28872.814 sec)\n",
      "Step 537: loss = 2817.96 (28927.737 sec)\n",
      "Step 538: loss = 2813.63 (28982.292 sec)\n",
      "Step 539: loss = 2809.53 (29031.763 sec)\n",
      "Step 540: loss = 2805.63 (29085.151 sec)\n",
      "Step 541: loss = 2802.24 (29134.322 sec)\n",
      "Step 542: loss = 2796.89 (29192.199 sec)\n",
      "Step 543: loss = 2791.23 (29245.070 sec)\n",
      "Step 544: loss = 2787.14 (29299.821 sec)\n",
      "Step 545: loss = 2782.10 (29357.903 sec)\n",
      "Step 546: loss = 2777.44 (29415.212 sec)\n",
      "Step 547: loss = 2772.15 (29474.748 sec)\n",
      "Step 548: loss = 2767.29 (29534.598 sec)\n",
      "Step 549: loss = 2763.92 (29583.003 sec)\n",
      "Step 550: loss = 2760.56 (29638.328 sec)\n",
      "Step 551: loss = 2756.08 (29689.727 sec)\n",
      "Step 552: loss = 2752.38 (29744.497 sec)\n",
      "Step 553: loss = 2749.26 (29801.000 sec)\n",
      "Step 554: loss = 2743.30 (29860.112 sec)\n",
      "Step 555: loss = 2739.47 (29910.186 sec)\n",
      "Step 556: loss = 2735.70 (29960.859 sec)\n",
      "Step 557: loss = 2732.43 (30012.004 sec)\n",
      "Step 558: loss = 2727.76 (30062.077 sec)\n",
      "Step 559: loss = 2722.79 (30113.399 sec)\n",
      "Step 560: loss = 2718.55 (30170.170 sec)\n",
      "Step 561: loss = 2712.81 (30218.959 sec)\n",
      "Step 562: loss = 2708.24 (30276.646 sec)\n",
      "Step 563: loss = 2703.50 (30333.202 sec)\n",
      "Step 564: loss = 2699.65 (30386.084 sec)\n",
      "Step 565: loss = 2693.77 (30437.368 sec)\n",
      "Step 566: loss = 2687.86 (30496.673 sec)\n",
      "Step 567: loss = 2684.11 (30555.691 sec)\n",
      "Step 568: loss = 2679.51 (30614.906 sec)\n",
      "Step 569: loss = 2673.79 (30674.118 sec)\n",
      "Step 570: loss = 2667.86 (30723.229 sec)\n",
      "Step 571: loss = 2664.77 (30778.472 sec)\n",
      "Step 572: loss = 2661.43 (30834.650 sec)\n",
      "Step 573: loss = 2657.52 (30894.522 sec)\n",
      "Step 574: loss = 2651.92 (30942.639 sec)\n",
      "Step 575: loss = 2648.37 (30995.962 sec)\n",
      "Step 576: loss = 2643.53 (31054.212 sec)\n",
      "Step 577: loss = 2638.18 (31106.867 sec)\n",
      "Step 578: loss = 2634.67 (31160.549 sec)\n",
      "Step 579: loss = 2629.43 (31220.401 sec)\n",
      "Step 580: loss = 2624.44 (31277.303 sec)\n",
      "Step 581: loss = 2621.14 (31328.742 sec)\n",
      "Step 582: loss = 2615.21 (31381.283 sec)\n",
      "Step 583: loss = 2611.82 (31438.340 sec)\n",
      "Step 584: loss = 2606.86 (31491.789 sec)\n",
      "Step 585: loss = 2601.83 (31549.957 sec)\n",
      "Step 586: loss = 2595.84 (31600.187 sec)\n",
      "Step 587: loss = 2591.07 (31657.320 sec)\n",
      "Step 588: loss = 2587.55 (31705.401 sec)\n",
      "Step 589: loss = 2583.06 (31757.090 sec)\n",
      "Step 590: loss = 2578.36 (31807.444 sec)\n",
      "Step 591: loss = 2574.36 (31863.413 sec)\n",
      "Step 592: loss = 2569.51 (31920.734 sec)\n",
      "Step 593: loss = 2563.57 (31978.859 sec)\n",
      "Step 594: loss = 2558.81 (32032.304 sec)\n",
      "Step 595: loss = 2554.26 (32084.227 sec)\n",
      "Step 596: loss = 2551.20 (32138.602 sec)\n",
      "Step 597: loss = 2547.95 (32190.076 sec)\n",
      "Step 598: loss = 2543.62 (32243.449 sec)\n",
      "Step 599: loss = 2538.97 (32300.980 sec)\n",
      "Step 600: loss = 2534.35 (32357.688 sec)\n",
      "Step 601: loss = 2530.09 (32411.537 sec)\n",
      "Step 602: loss = 2525.17 (32469.569 sec)\n",
      "Step 603: loss = 2521.39 (32525.567 sec)\n",
      "Step 604: loss = 2517.01 (32577.214 sec)\n",
      "Step 605: loss = 2513.77 (32635.087 sec)\n",
      "Step 606: loss = 2507.77 (32694.358 sec)\n",
      "Step 607: loss = 2502.35 (32749.654 sec)\n",
      "Step 608: loss = 2497.98 (32804.479 sec)\n",
      "Step 609: loss = 2492.74 (32860.514 sec)\n",
      "Step 610: loss = 2488.40 (32910.600 sec)\n",
      "Step 611: loss = 2484.59 (32964.809 sec)\n",
      "Step 612: loss = 2479.49 (33024.032 sec)\n",
      "Step 613: loss = 2474.98 (33078.400 sec)\n",
      "Step 614: loss = 2471.74 (33127.998 sec)\n",
      "Step 615: loss = 2466.58 (33185.447 sec)\n",
      "Step 616: loss = 2460.91 (33235.928 sec)\n",
      "Step 617: loss = 2457.18 (33289.946 sec)\n",
      "Step 618: loss = 2453.29 (33342.903 sec)\n",
      "Step 619: loss = 2447.48 (33400.586 sec)\n",
      "Step 620: loss = 2444.24 (33450.070 sec)\n",
      "Step 621: loss = 2440.83 (33508.897 sec)\n",
      "Step 622: loss = 2435.27 (33561.460 sec)\n",
      "Step 623: loss = 2431.04 (33610.043 sec)\n",
      "Step 624: loss = 2427.72 (33663.874 sec)\n",
      "Step 625: loss = 2424.09 (33722.016 sec)\n",
      "Step 626: loss = 2421.04 (33777.363 sec)\n",
      "Step 627: loss = 2417.21 (33831.446 sec)\n",
      "Step 628: loss = 2411.54 (33888.107 sec)\n",
      "Step 629: loss = 2407.89 (33943.426 sec)\n",
      "Step 630: loss = 2402.96 (33994.980 sec)\n",
      "Step 631: loss = 2398.22 (34050.370 sec)\n",
      "Step 632: loss = 2393.95 (34105.433 sec)\n",
      "Step 633: loss = 2390.15 (34162.897 sec)\n",
      "Step 634: loss = 2385.63 (34216.101 sec)\n",
      "Step 635: loss = 2379.80 (34267.838 sec)\n",
      "Step 636: loss = 2374.97 (34327.316 sec)\n",
      "Step 637: loss = 2369.26 (34378.584 sec)\n",
      "Step 638: loss = 2366.17 (34434.853 sec)\n",
      "Step 639: loss = 2360.34 (34490.133 sec)\n",
      "Step 640: loss = 2355.51 (34542.694 sec)\n",
      "Step 641: loss = 2350.27 (34592.255 sec)\n",
      "Step 642: loss = 2344.92 (34650.824 sec)\n",
      "Step 643: loss = 2340.48 (34704.759 sec)\n",
      "Step 644: loss = 2336.62 (34762.658 sec)\n",
      "Step 645: loss = 2330.77 (34811.508 sec)\n",
      "Step 646: loss = 2325.35 (34865.621 sec)\n",
      "Step 647: loss = 2319.92 (34915.988 sec)\n",
      "Step 648: loss = 2316.27 (34971.218 sec)\n",
      "Step 649: loss = 2310.94 (35025.011 sec)\n",
      "Step 650: loss = 2306.66 (35078.093 sec)\n",
      "Step 651: loss = 2302.65 (35128.791 sec)\n",
      "Step 652: loss = 2299.40 (35180.184 sec)\n",
      "Step 653: loss = 2293.69 (35232.001 sec)\n",
      "Step 654: loss = 2290.16 (35280.432 sec)\n",
      "Step 655: loss = 2287.05 (35329.806 sec)\n",
      "Step 656: loss = 2281.13 (35379.130 sec)\n",
      "Step 657: loss = 2277.26 (35431.271 sec)\n",
      "Step 658: loss = 2273.19 (35487.582 sec)\n",
      "Step 659: loss = 2269.95 (35542.006 sec)\n",
      "Step 660: loss = 2265.50 (35601.485 sec)\n",
      "Step 661: loss = 2261.77 (35659.242 sec)\n",
      "Step 662: loss = 2258.16 (35711.741 sec)\n",
      "Step 663: loss = 2252.67 (35770.423 sec)\n",
      "Step 664: loss = 2247.65 (35825.626 sec)\n",
      "Step 665: loss = 2243.58 (35882.357 sec)\n",
      "Step 666: loss = 2239.59 (35941.644 sec)\n",
      "Step 667: loss = 2234.72 (35995.386 sec)\n",
      "Step 668: loss = 2231.04 (36048.595 sec)\n",
      "Step 669: loss = 2225.79 (36099.770 sec)\n",
      "Step 670: loss = 2220.28 (36159.472 sec)\n",
      "Step 671: loss = 2214.94 (36212.099 sec)\n",
      "Step 672: loss = 2210.34 (36261.696 sec)\n",
      "Step 673: loss = 2205.93 (36310.086 sec)\n",
      "Step 674: loss = 2202.09 (36365.287 sec)\n",
      "Step 675: loss = 2196.45 (36423.468 sec)\n",
      "Step 676: loss = 2192.71 (36474.062 sec)\n",
      "Step 677: loss = 2187.47 (36529.233 sec)\n",
      "Step 678: loss = 2181.51 (36585.033 sec)\n",
      "Step 679: loss = 2177.03 (36640.200 sec)\n",
      "Step 680: loss = 2171.10 (36692.472 sec)\n",
      "Step 681: loss = 2166.08 (36743.352 sec)\n",
      "Step 682: loss = 2162.97 (36794.482 sec)\n",
      "Step 683: loss = 2159.85 (36851.905 sec)\n",
      "Step 684: loss = 2154.81 (36904.458 sec)\n",
      "Step 685: loss = 2149.54 (36960.732 sec)\n",
      "Step 686: loss = 2144.26 (37015.469 sec)\n",
      "Step 687: loss = 2140.32 (37075.047 sec)\n",
      "Step 688: loss = 2136.83 (37124.613 sec)\n",
      "Step 689: loss = 2131.68 (37183.741 sec)\n",
      "Step 690: loss = 2126.13 (37234.944 sec)\n",
      "Step 691: loss = 2121.40 (37284.135 sec)\n",
      "Step 692: loss = 2116.06 (37334.111 sec)\n",
      "Step 693: loss = 2112.46 (37382.709 sec)\n",
      "Step 694: loss = 2107.92 (37432.084 sec)\n",
      "Step 695: loss = 2103.40 (37481.432 sec)\n",
      "Step 696: loss = 2098.91 (37532.283 sec)\n",
      "Step 697: loss = 2095.35 (37585.480 sec)\n",
      "Step 698: loss = 2089.77 (37636.373 sec)\n",
      "Step 699: loss = 2084.54 (37693.061 sec)\n",
      "Step 700: loss = 2081.39 (37745.893 sec)\n",
      "Step 701: loss = 2076.09 (37803.310 sec)\n",
      "Step 702: loss = 2072.93 (37861.140 sec)\n",
      "Step 703: loss = 2068.82 (37917.007 sec)\n",
      "Step 704: loss = 2062.84 (37966.950 sec)\n",
      "Step 705: loss = 2059.25 (38023.103 sec)\n",
      "Step 706: loss = 2053.43 (38080.490 sec)\n",
      "Step 707: loss = 2049.44 (38137.220 sec)\n",
      "Step 708: loss = 2045.89 (38196.494 sec)\n",
      "Step 709: loss = 2040.84 (38255.213 sec)\n",
      "Step 710: loss = 2035.69 (38309.280 sec)\n",
      "Step 711: loss = 2031.87 (38362.856 sec)\n",
      "Step 712: loss = 2026.86 (38422.553 sec)\n",
      "Step 713: loss = 2022.43 (38473.762 sec)\n",
      "Step 714: loss = 2018.81 (38528.134 sec)\n",
      "Step 715: loss = 2015.18 (38584.492 sec)\n",
      "Step 716: loss = 2012.09 (38637.861 sec)\n",
      "Step 717: loss = 2008.29 (38692.049 sec)\n",
      "Step 718: loss = 2004.02 (38748.958 sec)\n",
      "Step 719: loss = 2000.33 (38799.322 sec)\n",
      "Step 720: loss = 1994.40 (38855.847 sec)\n",
      "Step 721: loss = 1990.71 (38907.817 sec)\n",
      "Step 722: loss = 1987.51 (38963.550 sec)\n",
      "Step 723: loss = 1983.44 (39017.648 sec)\n",
      "Step 724: loss = 1978.46 (39077.611 sec)\n",
      "Step 725: loss = 1974.85 (39130.981 sec)\n",
      "Step 726: loss = 1971.54 (39186.673 sec)\n",
      "Step 727: loss = 1966.25 (39243.704 sec)\n",
      "Step 728: loss = 1962.63 (39298.971 sec)\n",
      "Step 729: loss = 1956.69 (39358.186 sec)\n",
      "Step 730: loss = 1952.23 (39417.863 sec)\n",
      "Step 731: loss = 1949.06 (39476.028 sec)\n",
      "Step 732: loss = 1945.00 (39526.641 sec)\n",
      "Step 733: loss = 1939.83 (39577.717 sec)\n",
      "Step 734: loss = 1936.52 (39633.523 sec)\n",
      "Step 735: loss = 1931.38 (39691.737 sec)\n",
      "Step 736: loss = 1928.26 (39745.234 sec)\n",
      "Step 737: loss = 1924.36 (39801.802 sec)\n",
      "Step 738: loss = 1920.69 (39857.927 sec)\n",
      "Step 739: loss = 1915.58 (39908.966 sec)\n",
      "Step 740: loss = 1911.36 (39960.396 sec)\n",
      "Step 741: loss = 1908.23 (40010.484 sec)\n",
      "Step 742: loss = 1903.93 (40068.495 sec)\n",
      "Step 743: loss = 1899.29 (40123.087 sec)\n",
      "Step 744: loss = 1894.01 (40172.563 sec)\n",
      "Step 745: loss = 1890.36 (40229.005 sec)\n",
      "Step 746: loss = 1884.54 (40279.851 sec)\n",
      "Step 747: loss = 1880.34 (40335.860 sec)\n",
      "Step 748: loss = 1877.20 (40386.463 sec)\n",
      "Step 749: loss = 1874.01 (40441.280 sec)\n",
      "Step 750: loss = 1869.02 (40489.444 sec)\n",
      "Step 751: loss = 1865.83 (40542.095 sec)\n",
      "Step 752: loss = 1861.50 (40597.108 sec)\n",
      "Step 753: loss = 1857.58 (40652.212 sec)\n",
      "Now_epoch is: 1\n",
      "Accuracy for law prediction is:  85.04290792421698\n",
      "Other metrics for law prediction is:  (85.04290792421698, 80.12282126685358, 85.59200026194095, 81.43587576336364, 85.48945235678492)\n",
      "\n",
      "\n",
      "Accuracy for accu prediction is:  87.6209875073118\n",
      "Other metrics for accu prediction is:  (87.6209875073118, 83.55392047291814, 82.71274558149736, 87.86879077610934, 86.84254937330527)\n",
      "\n",
      "\n",
      "Accuracy for time prediction is:  38.9368193527364\n",
      "Other metrics for time prediction is:  (38.9368193527364, 34.20735330241356, 33.68020274777557, 38.303034873287146, 38.13248681297143)\n",
      "\n",
      "\n",
      "Now_testing\n",
      "Accuracy for law prediction is:  81.06036673821511\n",
      "Other metrics for law prediction is:  (81.06036673821511, 80.66386110358894, 79.47997003153421, 78.84306536745764, 85.5198970320859)\n",
      "\n",
      "\n",
      "Accuracy for accu prediction is:  84.16600334520379\n",
      "Other metrics for accu prediction is:  (84.16600334520379, 80.9039555517571, 81.07580872665348, 82.55920623854708, 83.8636314202011)\n",
      "\n",
      "\n",
      "Accuracy for time prediction is:  36.260282477639365\n",
      "Other metrics for time prediction is:  (36.260282477639365, 35.38428603897323, 33.53769055733629, 34.63442039612735, 35.91140801357871)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    regualrizer = layers.l2_regularizer(0.)\n",
    "    fact_input = tf.placeholder(tf.int32, [batch_size, doc_len_fact, sent_len_fact], name='fact')\n",
    "    law_labels = tf.placeholder(tf.int32, [batch_size], name='fact')\n",
    "    accu_labels = tf.placeholder(tf.int32, [batch_size], name='fact')\n",
    "    time_labels = tf.placeholder(tf.int32, [batch_size], name='fact')\n",
    "\n",
    "    law_input, graph_list_1, graph_membership, neigh_index = get_law_graph(law_relation_threshold, '../data/w2id_thulac.pkl', 15, 100)\n",
    "\n",
    "    fact_mask = tf.cast(tf.cast(fact_input - word2id_dict['BLANK'], tf.bool), tf.float32)\n",
    "    fact_sent_len = tf.reduce_sum(fact_mask, -1)\n",
    "    fact_doc_mask = tf.cast(tf.cast(fact_sent_len, tf.bool), tf.float32)\n",
    "    fact_doc_len = tf.reduce_sum(fact_doc_mask, -1)\n",
    "\n",
    "    law_mask = tf.cast(tf.cast(law_input - word2id_dict['BLANK'], tf.bool), tf.float32)\n",
    "    law_sent_len = tf.reduce_sum(law_mask, -1)\n",
    "    law_doc_mask = tf.cast(tf.cast(law_sent_len, tf.bool), tf.float32)\n",
    "    law_doc_len = tf.reduce_sum(law_doc_mask, -1)\n",
    "\n",
    "   \n",
    "    fact_description = tf.nn.embedding_lookup(word_embedding, fact_input)\n",
    "    law_description = tf.nn.embedding_lookup(word_embedding, law_input)\n",
    "\n",
    "    max_graph = len(graph_list_1)\n",
    "    deg_list = [len(neigh_index[i]) for i in range(n_law)]\n",
    "    graph_list = list(zip(*graph_membership))[1]\n",
    "\n",
    "    gold_matrix_law = tf.one_hot(law_labels, 103, dtype=tf.float32)\n",
    "    gold_matrix_accu = tf.one_hot(accu_labels, 119, dtype=tf.float32)\n",
    "    gold_matrix_time = tf.one_hot(time_labels, 12, dtype=tf.float32)\n",
    "\n",
    "    #############----------------------###################\n",
    "    graph_label = tf.dynamic_partition(tf.transpose(gold_matrix_law, [1, 0]), graph_list, max_graph)  # size: [batch_size, graph_num, N_each_graph])\n",
    "    label = []\n",
    "    for i in range(max_graph):\n",
    "        label.append(tf.reduce_sum(graph_label[i], 0, keepdims=True))\n",
    "\n",
    "    graph_label = tf.transpose(tf.concat(label, 0), [1, 0])  # size: [batch_size, graph_num]\n",
    "    #############----------------------###################\n",
    "\n",
    "    neigh_index = sorted(neigh_index.items(), key=lambda x: len(x[1]))\n",
    "    max_deg = len(neigh_index[-1][1])\n",
    "    t = 0\n",
    "    adj_list = [[]]\n",
    "    for i in range(n_law):\n",
    "        each = neigh_index[i]\n",
    "        if len(each[1]) != t:\n",
    "            for j in range(t, len(each[1])):\n",
    "                adj_list.append([])\n",
    "            t = len(each[1])\n",
    "        adj_list[-1].append(each[1])\n",
    "\n",
    "    u_aw = tf.get_variable('u_aw', shape=[1, lstm_size * 2], initializer=layers.xavier_initializer())\n",
    "    u_as = tf.get_variable('u_as', shape=[1, lstm_size * 2], initializer=layers.xavier_initializer())\n",
    "\n",
    "    Fully_atten_sent_1 = keras.layers.Dense(lstm_size * 2, name='Fully_atten_sent_1')\n",
    "    Fully_atten_doc_1 = keras.layers.Dense(lstm_size * 2, name='Fully_atten_doc_1')\n",
    "\n",
    "    model = keras.Sequential([keras.layers.Bidirectional(keras.layers.GRU(lstm_size, return_sequences=True),merge_mode='concat')])\n",
    "    rep_law = run_model(law_description, law_mask, model)\n",
    "    rep_fact = run_model(fact_description, fact_mask, model)\n",
    "\n",
    "    # rep_law_ = tf.reduce_mean(rep_law, -2)\n",
    "    # rep_fact_ = tf.reduce_mean(rep_fact, -2)\n",
    "\n",
    "    rep_law_, _ = atten_encoder_mask(u_aw, rep_law, Fully_atten_sent_1, law_mask, K_ori=True)\n",
    "    rep_fact_, _ = atten_encoder_mask(u_aw, rep_fact, Fully_atten_sent_1, fact_mask, K_ori=True)\n",
    "\n",
    "    model_1 = keras.Sequential([keras.layers.Bidirectional(keras.layers.GRU(lstm_size, return_sequences=True), merge_mode='concat')])\n",
    "    rep_law_1 = run_model(rep_law_, law_doc_mask, model_1)\n",
    "    rep_fact_1 = run_model(rep_fact_, fact_doc_mask, model_1)\n",
    "\n",
    "    # rep_law_1 = tf.reduce_mean(rep_law_1, -2)\n",
    "    # rep_fact_1 = tf.reduce_mean(rep_fact_1, -2)\n",
    "    rep_law_1, _ = atten_encoder_mask(u_as, rep_law_1, Fully_atten_doc_1, law_doc_mask, K_ori=True)\n",
    "    rep_fact_1, _ = atten_encoder_mask(u_as, rep_fact_1, Fully_atten_doc_1, fact_doc_mask, K_ori=True)\n",
    "\n",
    "    with tf.name_scope('interaction'):\n",
    "\n",
    "        indices = tf.dynamic_partition(tf.range(n_law), deg_list, max_deg + 1)\n",
    "        law_representation = tf.dynamic_partition(rep_law_1, deg_list, max_deg + 1)\n",
    "        ######--------------graph convolution operator------------######\n",
    "\n",
    "        '''\n",
    "        law_repr : law representation           \n",
    "        '''\n",
    "        W_similar = tf.get_variable('W_similar', shape=[2 * lstm_size * 2, lstm_size * 2],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "        B_similar = tf.get_variable('B_similar', shape=[1, lstm_size * 2],\n",
    "                                    initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "        Full_inter_1 = keras.layers.Dense(lstm_size *2)\n",
    "        article_new_list = [tf.nn.tanh(law_representation[0])]\n",
    "\n",
    "        for i in range(1, max_deg + 1):\n",
    "            if i not in deg_list:\n",
    "                article_new_list.append(tf.nn.tanh(law_representation[i]))\n",
    "                continue\n",
    "\n",
    "            neigh_articles = tf.gather(rep_law_1, adj_list[i])  # size: n * deg * law_size\n",
    "            neigh_articles = tf.reshape(neigh_articles, [-1, i, lstm_size * 2])\n",
    "\n",
    "            article = law_representation[i]  # size: n * law_size\n",
    "\n",
    "            article_1 = tf.transpose(tf.reshape(tf.tile(article, [i, 1]), [i, -1, lstm_size * 2]), [1, 0, 2])\n",
    "            interaction_vec = tf.concat([article_1, neigh_articles], axis=-1)  # size: [n, deg, law_size *2]\n",
    "            neigh_articles = tf.reduce_mean(\n",
    "                tf.tensordot(interaction_vec, W_similar, [2, 0]) + tf.expand_dims(B_similar, axis=0),\n",
    "                1)  # [n, law_size]\n",
    "\n",
    "            new_article = tf.nn.tanh(Full_inter_1(article - neigh_articles))\n",
    "            article_new_list.append(new_article)  # [max_degree, N, law_size]\n",
    "\n",
    "        law_conv = tf.dynamic_stitch(indices, article_new_list)  # size: [183, law_size]\n",
    "\n",
    "        # W_similar_1 = tf.get_variable('W_similar_1', shape=[2 * lstm_size * 2, lstm_size * 2],\n",
    "        #                             initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "        # B_similar_1 = tf.get_variable('B_similar_1', shape=[1, lstm_size * 2],\n",
    "        #                             initializer=tf.contrib.layers.xavier_initializer(), trainable=True)\n",
    "        # Full_inter_2 = keras.layers.Dense(lstm_size * 2)\n",
    "\n",
    "        article_new_list_1 = [tf.nn.tanh(article_new_list[0])]\n",
    "        for i in range(1, max_deg + 1):\n",
    "            if i not in deg_list:\n",
    "                article_new_list_1.append(tf.nn.tanh(article_new_list[i]))\n",
    "                continue\n",
    "\n",
    "            neigh_articles = tf.gather(law_conv, adj_list[i])  # size: n * deg * law_size\n",
    "            neigh_articles = tf.reshape(neigh_articles, [-1, i, lstm_size * 2])\n",
    "\n",
    "            article = article_new_list[i]  # size: n * law_size\n",
    "\n",
    "            article_1 = tf.transpose(tf.reshape(tf.tile(article, [i, 1]), [i, -1, lstm_size * 2]), [1, 0, 2])\n",
    "            interaction_vec = tf.concat([article_1, neigh_articles], axis=-1)  # size: [n, deg, law_size *2]\n",
    "            neigh_articles = tf.reduce_mean(\n",
    "                tf.tensordot(interaction_vec, W_similar, [2, 0]) + tf.expand_dims(B_similar, axis=0),\n",
    "                1)  # [n, law_size]\n",
    "\n",
    "            new_article = tf.nn.tanh(Full_inter_1(article - neigh_articles))\n",
    "            article_new_list_1.append(new_article)  # [max_degree, N, law_size]\n",
    "        law_conv = tf.dynamic_stitch(indices, article_new_list_1)\n",
    "\n",
    "        law_representation = tf.dynamic_partition(law_conv, graph_list, max_graph)  # size: [graph_num, n, law_size]\n",
    "        # size: [graph_num, N, law_size]\n",
    "        atten_list = []\n",
    "        for i in range(max_graph):\n",
    "            u = tf.reduce_max(law_representation[i], 0)  # law_representation[i]: [n, law_size]\n",
    "            u_2 = tf.reduce_min(law_representation[i], 0)\n",
    "            atten_list.append(tf.concat([u, u_2], -1))  # size: [graph_num, 2*law_size] whether this u can use attention to get\n",
    "\n",
    "    with tf.name_scope('law_re_encoder'):\n",
    "        law_u = tf.gather(atten_list, graph_list)  # size:[183, law_size]; law: [183, x, y, word_size]\n",
    "        Fully_connected_1 = keras.layers.Dense(lstm_size *2)\n",
    "        Fully_connected_2 = keras.layers.Dense(lstm_size *2)\n",
    "        u_law_w = tf.reshape(Fully_connected_1(law_u), [-1, 1, 1, lstm_size *2])\n",
    "        u_law_s = tf.reshape(Fully_connected_2(law_u), [-1, 1, lstm_size *2])\n",
    "\n",
    "        Fully_atten_sent_2 = keras.layers.Dense(lstm_size *2, kernel_regularizer=regualrizer)\n",
    "        Fully_atten_doc_2 = keras.layers.Dense(lstm_size *2, kernel_regularizer=regualrizer)\n",
    "\n",
    "        # model_2 = keras.Sequential(\n",
    "        #     [keras.layers.Bidirectional(keras.layers.LSTM(lstm_size, return_sequences=True), merge_mode='concat')])\n",
    "        # rep_law = run_model(law_description, law_mask, model_2)\n",
    "        rep_law, _ = atten_encoder_mask(u_law_w, rep_law, Fully_atten_sent_2, law_mask, K_ori=True)\n",
    "\n",
    "        model_3 = keras.Sequential(\n",
    "            [keras.layers.Bidirectional(keras.layers.GRU(lstm_size, return_sequences=True), merge_mode='concat')])\n",
    "        rep_law_2 = run_model(rep_law, law_doc_mask, model_3)\n",
    "        rep_law_2, _ = atten_encoder_mask(u_law_s, rep_law_2, Fully_atten_doc_2, law_doc_mask, K_ori=True)\n",
    "\n",
    "    with tf.name_scope('fact_re_encoder'):\n",
    "        Fully_connected_graph = keras.layers.Dense(max_graph)\n",
    "        fact_graph_choose_1 = Fully_connected_graph(rep_fact_1)\n",
    "        fact_graph_choose = tf.nn.softmax(fact_graph_choose_1, -1)\n",
    "\n",
    "        # graph_chose_loss = tf.losses.softmax_cross_entropy(graph_label, fact_graph_choose_1)\n",
    "        graph_chose_loss = tf.nn.softmax_cross_entropy_with_logits(logits=fact_graph_choose_1, labels=graph_label)\n",
    "        graph_chose_loss = tf.reduce_sum(graph_chose_loss)/128.0\n",
    "        graph_L = tf.arg_max(graph_label, -1)\n",
    "        correct_graph = tf.nn.in_top_k(fact_graph_choose_1, graph_L, 1)\n",
    "\n",
    "        #------------------------------------one-hot-----------------------------#\n",
    "\n",
    "        fact_graph_choose = tf.where(fact_graph_choose == tf.reduce_max(fact_graph_choose, -1), tf.ones_like(fact_graph_choose), tf.zeros_like(fact_graph_choose))\n",
    "\n",
    "        ###########################################################################\n",
    "\n",
    "        atten_tensor = tf.reshape(tf.concat(atten_list, 0), [-1, 2 * lstm_size * 2])\n",
    "        u_fact = fact_graph_choose @ atten_tensor\n",
    "\n",
    "        u_fact_w = tf.reshape(Fully_connected_1(u_fact), [-1, 1, 1, lstm_size * 2])\n",
    "        u_fact_s = tf.reshape(Fully_connected_2(u_fact), [-1, 1, lstm_size * 2])\n",
    "\n",
    "        # rep_fact = run_model(fact_description, fact_mask, model_2)\n",
    "        rep_fact, _ = atten_encoder_mask(u_fact_w, rep_fact, Fully_atten_sent_2, fact_mask, K_ori=True)\n",
    "\n",
    "        rep_fact_2 = run_model(rep_fact, fact_doc_mask, model_3)\n",
    "        rep_fact_2, _ = atten_encoder_mask(u_fact_s, rep_fact_2, Fully_atten_doc_2, fact_doc_mask, K_ori=True)\n",
    "\n",
    "        fact_repr = tf.concat([rep_fact_1, rep_fact_2], -1)  # size: [batch_size, 2 * fact_size]\n",
    "        law_repr = tf.concat([rep_law_1, rep_law_2], -1)  # size: [183 , 2 * law_size]\n",
    "\n",
    "    Full_law_1 = keras.layers.Dense(clr_fc1_size)\n",
    "    Full_law_2 = keras.layers.Dense(n_law)\n",
    "    law_output = Full_law_2(tf.nn.relu(Full_law_1(law_repr)))\n",
    "    loss_law_article = tf.losses.softmax_cross_entropy(tf.one_hot(tf.range(n_law), n_law), law_output)\n",
    "\n",
    "    decoder = LSTMDecoder(config, 103, 119)\n",
    "    output_task1, output_task2, output_task3 = decoder(fact_repr)\n",
    "    '''\n",
    "        output_task1: embeddings for law prediction\n",
    "        output_task2: embeddings for accu prediction\n",
    "        output_task3: embeddings for time prediction\n",
    "        '''\n",
    "\n",
    "    law_prob = tf.nn.softmax(output_task1, -1)\n",
    "    accu_prob = tf.nn.softmax(output_task2, -1)\n",
    "    time_prob = tf.nn.softmax(output_task3, -1)\n",
    "\n",
    "    law_predictions = tf.argmax(law_prob, 1)\n",
    "    accu_predictions = tf.argmax(accu_prob, 1)\n",
    "    time_predictions = tf.argmax(time_prob, 1)\n",
    "\n",
    "    loss_1 = tf.nn.softmax_cross_entropy_with_logits(logits=output_task1, labels=gold_matrix_law)\n",
    "    loss_2 = tf.nn.softmax_cross_entropy_with_logits(logits=output_task2, labels=gold_matrix_accu)\n",
    "    loss_3 = tf.nn.softmax_cross_entropy_with_logits(logits=output_task3, labels=gold_matrix_time)\n",
    "\n",
    "    law_loss = tf.reduce_sum(loss_1)\n",
    "    accu_loss = tf.reduce_sum(loss_2)\n",
    "    time_loss = tf.reduce_sum(loss_3)\n",
    "\n",
    "    # loss = (law_loss + accu_loss + time_loss) / batch_size + loss_law_article / 103.0\n",
    "    loss = (law_loss + accu_loss + time_loss) / batch_size + loss_law_article / 103.0 + 0.1 * graph_chose_loss\n",
    "    # loss = (law_loss + accu_loss + time_loss) / batch_size + 0.1 * graph_chose_loss\n",
    "    tf.add_to_collection('losses_1', tf.contrib.layers.l2_regularizer(0.0001)(loss_1))\n",
    "    tf.add_to_collection('losses_2', tf.contrib.layers.l2_regularizer(0.0001)(loss_2))\n",
    "    tf.add_to_collection('losses_3', tf.contrib.layers.l2_regularizer(0.0001)(loss_3))\n",
    "\n",
    "    loss_total = loss + tf.add_n(tf.get_collection('losses_1')) + tf.add_n(tf.get_collection('losses_2')) + tf.add_n(\n",
    "        tf.get_collection('losses_3'))\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=.9, beta2=.999, epsilon=1e-7)\n",
    "    train_op = optimizer.minimize(loss_total, global_step=global_step)\n",
    "\n",
    "    correct_law = tf.nn.in_top_k(output_task1, law_labels, 1)\n",
    "    correct_accu = tf.nn.in_top_k(output_task2, accu_labels, 1)\n",
    "    correct_time = tf.nn.in_top_k(output_task3, time_labels, 1)\n",
    "\n",
    "    ###########################-----------graph built over-----------###############################\n",
    "\n",
    "    initializer = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "    config_tf = tf.ConfigProto()\n",
    "    config_tf.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    config_tf.gpu_options.allow_growth = True\n",
    "\n",
    "    sess = tf.Session(config=config_tf)\n",
    "    sess.run(initializer)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    graph_chose_total = 0.0\n",
    "    ave_graph_acc = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    ############################--------------initialized over---------------########################\n",
    "    f_train = pk.load(open('../process_data_second/train_processed_thulac_Legal_basis.pkl', 'rb'))\n",
    "    f_valid = pk.load(open('../process_data_second/valid_processed_thulac_Legal_basis.pkl', 'rb'))\n",
    "    f_test = pk.load(open('../process_data_second/test_processed_thulac_Legal_basis.pkl', 'rb'))\n",
    "\n",
    "    train_step = int(len(f_train['fact_list']) / batch_size) + 1\n",
    "    lose_num_train = train_step * batch_size - len(f_train['fact_list'])\n",
    "\n",
    "    valid_step = int(len(f_valid['fact_list']) / batch_size) + 1\n",
    "    lose_num_valid = valid_step * batch_size - len(f_valid['fact_list'])\n",
    "\n",
    "    test_step = int(len(f_test['fact_list']) / batch_size) + 1\n",
    "    lose_num_test = test_step * batch_size - len(f_test['fact_list'])\n",
    "\n",
    "    fact_train = f_train['fact_list']\n",
    "    law_labels_train = f_train['law_label_lists']\n",
    "    accu_label_train = f_train['accu_label_lists']\n",
    "    term_train = f_train['term_lists']\n",
    "\n",
    "    if shuffle:\n",
    "        index = [i for i in range(len(f_train['term_lists']))]\n",
    "        random.shuffle(index)\n",
    "        fact_train = [fact_train[i] for i in index]\n",
    "        law_labels_train = [law_labels_train[i] for i in index]\n",
    "        accu_label_train = [accu_label_train[i] for i in index]\n",
    "        term_train = [term_train[i] for i in index]\n",
    "\n",
    "        for epoch in range(max_epoch):\n",
    "            for i in range(train_step):\n",
    "                if i == train_step - 1:\n",
    "                    inputs = np.array(fact_train[i * batch_size:] + fact_train[:lose_num_train], dtype='int32')\n",
    "                    law_labels_input = np.array(law_labels_train[i * batch_size:] + law_labels_train[:lose_num_train],\n",
    "                                                dtype='int32')\n",
    "                    accu_labels_input = np.array(accu_label_train[i * batch_size:] + accu_label_train[:lose_num_train],\n",
    "                                                 dtype='int32')\n",
    "                    time_labels_input = np.array(term_train[i * batch_size:] + term_train[:lose_num_train],\n",
    "                                                 dtype='int32')\n",
    "                else:\n",
    "                    inputs = np.array(fact_train[i * batch_size: (i + 1) * batch_size], dtype='int32')\n",
    "                    law_labels_input = np.array(law_labels_train[i * batch_size: (i + 1) * batch_size], dtype='int32')\n",
    "                    accu_labels_input = np.array(accu_label_train[i * batch_size: (i + 1) * batch_size], dtype='int32')\n",
    "                    time_labels_input = np.array(term_train[i * batch_size: (i + 1) * batch_size], dtype='int32')\n",
    "                \n",
    "                feed_dict = gen_dict(inputs, law_labels_input, accu_labels_input, time_labels_input)\n",
    "                correct_graph_ = sess.run([correct_graph], feed_dict=feed_dict)\n",
    "                loss_value, _, graph_chose_value= sess.run([loss_total, train_op, graph_chose_loss], feed_dict=feed_dict)\n",
    "                total_loss += loss_value\n",
    "                graph_chose_total += graph_chose_value\n",
    "                ave_graph_acc += np.sum(np.cast[np.int32](correct_graph_))/128.0\n",
    "\n",
    "                if (i + 1) == train_step:\n",
    "                    duration = time.time() - start_time\n",
    "                    start_time = time.time()\n",
    "                    print('Step %d: loss = %.2f (%.3f sec)' % (i, total_loss, duration))\n",
    "                    losses = total_loss\n",
    "                    total_loss = 0.0\n",
    "                    graph_chose_total = 0.0\n",
    "                    ave_graph_acc = 0.0\n",
    "            ############################----------the following is valid prediction-----------------###############################\n",
    "            predic_law, predic_accu, predic_time = [], [], []\n",
    "            y_law, y_accu, y_time = [], [], []\n",
    "            time_correct = []\n",
    "            loss_sum = 0\n",
    "            total_tags = 0.0\n",
    "            correct_tags_law = 0\n",
    "            correct_tags_accu = 0\n",
    "            correct_tags_time = 0\n",
    "            for i in range(valid_step):\n",
    "                if i == valid_step - 1:\n",
    "                    inputs = np.array(f_valid['fact_list'][i * batch_size:] + f_valid['fact_list'][:lose_num_valid],\n",
    "                                      dtype='int32')\n",
    "                    law_labels_input = np.array(\n",
    "                        f_valid['law_label_lists'][i * batch_size:] + f_valid['law_label_lists'][:lose_num_valid],\n",
    "                        dtype='int32')\n",
    "                    accu_labels_input = np.array(\n",
    "                        f_valid['accu_label_lists'][i * batch_size:] + f_valid['accu_label_lists'][:lose_num_valid],\n",
    "                        dtype='int32')\n",
    "                    time_labels_input = np.array(\n",
    "                        f_valid['term_lists'][i * batch_size:] + f_valid['term_lists'][:lose_num_valid],\n",
    "                        dtype='int32')\n",
    "                else:\n",
    "                    inputs = np.array(f_valid['fact_list'][i * batch_size: (i + 1) * batch_size], dtype='int32')\n",
    "                    law_labels_input = np.array(f_valid['law_label_lists'][i * batch_size: (i + 1) * batch_size],\n",
    "                                                dtype='int32')\n",
    "                    accu_labels_input = np.array(f_valid['accu_label_lists'][i * batch_size: (i + 1) * batch_size],\n",
    "                                                 dtype='int32')\n",
    "                    time_labels_input = np.array(f_valid['term_lists'][i * batch_size: (i + 1) * batch_size],\n",
    "                                                 dtype='int32')\n",
    "\n",
    "                feed_dict_valid = gen_dict(inputs, law_labels_input, accu_labels_input, time_labels_input)\n",
    "                num_y = batch_size\n",
    "                if i + 1 == valid_step:\n",
    "                    num_y = batch_size - lose_num_valid\n",
    "\n",
    "                total_tags += num_y\n",
    "                correct_law_, correct_accu_, correct_time_, predic_law_, predic_accu_, predic_time_, y_law_, y_accu_, y_time_ = sess.run(\n",
    "                    (correct_law, correct_accu, correct_time, law_predictions, accu_predictions, time_predictions,\n",
    "                     law_labels, accu_labels, time_labels), feed_dict=feed_dict_valid)\n",
    "\n",
    "                predic_law += list(predic_law_[:num_y])\n",
    "                predic_accu += list(predic_accu_[:num_y])\n",
    "                predic_time += list(predic_time_[:num_y])\n",
    "\n",
    "                y_law += list(y_law_[:num_y])\n",
    "                y_accu += list(y_accu_[:num_y])\n",
    "                y_time += list(y_time_[:num_y])\n",
    "                time_correct += list(correct_time_[:num_y])\n",
    "\n",
    "                correct_tags_law += np.sum(np.cast[np.int32](correct_law_[:num_y]))\n",
    "                correct_tags_accu += np.sum(np.cast[np.int32](correct_accu_[:num_y]))\n",
    "                correct_tags_time += np.sum(np.cast[np.int32](correct_time_[:num_y]))\n",
    "\n",
    "            prediction = [predic_law, predic_accu, predic_time]\n",
    "            y = [y_law, y_accu, y_time]\n",
    "            correct_tags = [correct_tags_law, correct_tags_accu, correct_tags_time]\n",
    "            accuracy, metric = evaluation_multitask(y, prediction, 3, correct_tags, total_tags)\n",
    "            print('Now_epoch is: {}'.format(epoch))\n",
    "            for i in range(3):\n",
    "                print('Accuracy for {} prediction is: '.format(task[i]), accuracy[i])\n",
    "                print('Other metrics for {} prediction is: '.format(task[i]), metric[i])\n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "            ############################----------the following is valid prediction-----------------###############################\n",
    "            predic_law, predic_accu, predic_time = [], [], []\n",
    "            y_law, y_accu, y_time = [], [], []\n",
    "            time_correct = []\n",
    "            loss_sum = 0\n",
    "            total_tags = 0.0\n",
    "            correct_tags_law = 0\n",
    "            correct_tags_accu = 0\n",
    "            correct_tags_time = 0\n",
    "            for i in range(test_step):\n",
    "                if i == test_step - 1:\n",
    "                    inputs = np.array(f_test['fact_list'][i * batch_size:] + f_test['fact_list'][:lose_num_test],\n",
    "                                      dtype='int32')\n",
    "                    law_labels_input = np.array(\n",
    "                        f_test['law_label_lists'][i * batch_size:] + f_test['law_label_lists'][:lose_num_test],\n",
    "                        dtype='int32')\n",
    "                    accu_labels_input = np.array(\n",
    "                        f_test['accu_label_lists'][i * batch_size:] + f_test['accu_label_lists'][:lose_num_test],\n",
    "                        dtype='int32')\n",
    "                    time_labels_input = np.array(\n",
    "                        f_test['term_lists'][i * batch_size:] + f_test['term_lists'][:lose_num_test],\n",
    "                        dtype='int32')\n",
    "                else:\n",
    "                    inputs = np.array(f_test['fact_list'][i * batch_size: (i + 1) * batch_size], dtype='int32')\n",
    "                    law_labels_input = np.array(f_test['law_label_lists'][i * batch_size: (i + 1) * batch_size],\n",
    "                                                dtype='int32')\n",
    "                    accu_labels_input = np.array(f_test['accu_label_lists'][i * batch_size: (i + 1) * batch_size],\n",
    "                                                 dtype='int32')\n",
    "                    time_labels_input = np.array(f_test['term_lists'][i * batch_size: (i + 1) * batch_size],\n",
    "                                                 dtype='int32')\n",
    "\n",
    "                feed_dict_test = gen_dict(inputs, law_labels_input, accu_labels_input, time_labels_input)\n",
    "                num_y = batch_size\n",
    "                if i + 1 == test_step:\n",
    "                    num_y = batch_size - lose_num_test\n",
    "\n",
    "                total_tags += num_y\n",
    "                correct_law_, correct_accu_, correct_time_, predic_law_, predic_accu_, predic_time_, y_law_, y_accu_, y_time_ = sess.run(\n",
    "                    (correct_law, correct_accu, correct_time, law_predictions, accu_predictions, time_predictions,\n",
    "                     law_labels, accu_labels, time_labels), feed_dict=feed_dict_test)\n",
    "\n",
    "                predic_law += list(predic_law_[:num_y])\n",
    "                predic_accu += list(predic_accu_[:num_y])\n",
    "                predic_time += list(predic_time_[:num_y])\n",
    "\n",
    "                y_law += list(y_law_[:num_y])\n",
    "                y_accu += list(y_accu_[:num_y])\n",
    "                y_time += list(y_time_[:num_y])\n",
    "                time_correct += list(correct_time_[:num_y])\n",
    "\n",
    "                correct_tags_law += np.sum(np.cast[np.int32](correct_law_[:num_y]))\n",
    "                correct_tags_accu += np.sum(np.cast[np.int32](correct_accu_[:num_y]))\n",
    "                correct_tags_time += np.sum(np.cast[np.int32](correct_time_[:num_y]))\n",
    "\n",
    "            prediction = [predic_law, predic_accu, predic_time]\n",
    "            y = [y_law, y_accu, y_time]\n",
    "            correct_tags = [correct_tags_law, correct_tags_accu, correct_tags_time]\n",
    "\n",
    "            accuracy, metric = evaluation_multitask(y, prediction, 3, correct_tags, total_tags)\n",
    "            print('Now_testing')\n",
    "            for i in range(3):\n",
    "                print('Accuracy for {} prediction is: '.format(task[i]), accuracy[i])\n",
    "                print('Other metrics for {} prediction is: '.format(task[i]), metric[i])\n",
    "\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle as pk\n",
    "f_train = pk.load(open('../process_data_second/train_processed_thulac_Legal_basis.pkl', 'rb'))\n",
    "f_valid = pk.load(open('../process_data_second/valid_processed_thulac_Legal_basis.pkl', 'rb'))\n",
    "f_test = pk.load(open('../process_data_second/test_processed_thulac_Legal_basis.pkl', 'rb'))\n",
    "\n",
    "train_step = int(len(f_train['fact_list']) / 128) + 1\n",
    "train_step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
